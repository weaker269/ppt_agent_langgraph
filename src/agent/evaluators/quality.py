"""
å¹»ç¯ç‰‡è´¨é‡è¯„ä¼°å™¨

å®ç°å¤šç»´åº¦çš„å¹»ç¯ç‰‡è´¨é‡è¯„åˆ†ï¼ŒåŒ…æ‹¬å†…å®¹é€»è¾‘æ€§ã€ä¸»é¢˜ç›¸å…³æ€§ã€è¯­è¨€è´¨é‡å’Œè§†è§‰å¸ƒå±€ã€‚
æ”¯æŒç”Ÿæˆè¯¦ç»†çš„ç¼ºé™·åˆ†æå’Œä¼˜åŒ–å»ºè®®ã€‚
"""

import logging
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
from enum import Enum

from langchain_openai import ChatOpenAI
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.schema import HumanMessage

from ..state import SlideContent, PresentationOutline
from ..utils import ConfigManager

logger = logging.getLogger(__name__)


class QualityDimension(Enum):
    """è´¨é‡è¯„ä¼°ç»´åº¦"""
    LOGIC = "logic"  # å†…å®¹é€»è¾‘æ€§
    RELEVANCE = "relevance"  # ä¸»é¢˜ç›¸å…³æ€§
    LANGUAGE = "language"  # è¯­è¨€è¡¨è¾¾è´¨é‡
    LAYOUT = "layout"  # è§†è§‰å¸ƒå±€åˆç†æ€§


@dataclass
class QualityScore:
    """è´¨é‡è¯„åˆ†ç»“æœ"""
    total_score: float  # æ€»åˆ† (0-100)
    dimension_scores: Dict[str, float]  # å„ç»´åº¦å¾—åˆ†
    pass_threshold: bool  # æ˜¯å¦è¾¾åˆ°åŠæ ¼çº¿
    confidence: float  # è¯„åˆ†ç½®ä¿¡åº¦


@dataclass
class OptimizationSuggestion:
    """ä¼˜åŒ–å»ºè®®"""
    dimension: str  # éœ€è¦ä¼˜åŒ–çš„ç»´åº¦
    issue_description: str  # é—®é¢˜æè¿°
    suggestion: str  # å…·ä½“å»ºè®®
    priority: str  # ä¼˜å…ˆçº§ (high/medium/low)


class QualityEvaluator:
    """å¹»ç¯ç‰‡è´¨é‡è¯„ä¼°å™¨"""
    
    def __init__(self, model_provider: str = "openai"):
        """
        åˆå§‹åŒ–è´¨é‡è¯„ä¼°å™¨
        
        Args:
            model_provider: AIæ¨¡å‹æä¾›å•† ("openai" æˆ– "google")
        """
        self.config = ConfigManager()
        self.model_provider = model_provider
        self.llm = self._initialize_model()
        
        # é…ç½®å‚æ•°
        self.quality_threshold = float(self.config.get("QUALITY_THRESHOLD", "85"))
        self.max_retry_count = int(self.config.get("MAX_REFLECTION_RETRY", "3"))
        self.reflection_dimensions = self.config.get(
            "REFLECTION_DIMENSIONS", 
            "logic,relevance,language,layout"
        ).split(",")
        
        # ç»´åº¦æƒé‡é…ç½®
        self.dimension_weights = {
            "logic": 0.3,      # é€»è¾‘æ€§æƒé‡30%
            "relevance": 0.25, # ç›¸å…³æ€§æƒé‡25%
            "language": 0.25,  # è¯­è¨€è´¨é‡æƒé‡25%
            "layout": 0.2      # å¸ƒå±€æƒé‡20%
        }
        
        logger.info(f"è´¨é‡è¯„ä¼°å™¨åˆå§‹åŒ–å®Œæˆï¼Œé˜ˆå€¼: {self.quality_threshold}, æœ€å¤§é‡è¯•: {self.max_retry_count}")

    def _initialize_model(self):
        """åˆå§‹åŒ–AIæ¨¡å‹"""
        try:
            if self.model_provider == "openai":
                return ChatOpenAI(
                    model=self.config.get("OPENAI_MODEL", "gpt-3.5-turbo"),
                    temperature=0.3,  # è¾ƒä½çš„æ¸©åº¦ä»¥è·å¾—æ›´ä¸€è‡´çš„è¯„åˆ†
                    timeout=int(self.config.get("MODEL_TIMEOUT", "60"))
                )
            elif self.model_provider == "google":
                return ChatGoogleGenerativeAI(
                    model=self.config.get("GOOGLE_MODEL", "gemini-pro"),
                    temperature=0.3,
                    timeout=int(self.config.get("MODEL_TIMEOUT", "60"))
                )
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹æä¾›å•†: {self.model_provider}")
        except Exception as e:
            logger.error(f"æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            raise

    def evaluate_slide(
        self, 
        slide: SlideContent, 
        outline: PresentationOutline,
        context_slides: Optional[List[SlideContent]] = None
    ) -> Tuple[QualityScore, List[OptimizationSuggestion]]:
        """
        è¯„ä¼°å•å¼ å¹»ç¯ç‰‡çš„è´¨é‡
        
        Args:
            slide: è¦è¯„ä¼°çš„å¹»ç¯ç‰‡
            outline: æ¼”ç¤ºå¤§çº²
            context_slides: ä¸Šä¸‹æ–‡å¹»ç¯ç‰‡åˆ—è¡¨ï¼ˆç”¨äºä¸€è‡´æ€§æ£€æŸ¥ï¼‰
            
        Returns:
            Tuple[QualityScore, List[OptimizationSuggestion]]: è´¨é‡è¯„åˆ†å’Œä¼˜åŒ–å»ºè®®
        """
        try:
            logger.info(f"å¼€å§‹è¯„ä¼°å¹»ç¯ç‰‡: {slide.title}")
            
            # æ„å»ºè¯„ä¼°æç¤ºè¯
            evaluation_prompt = self._build_evaluation_prompt(slide, outline, context_slides)
            
            # è°ƒç”¨AIæ¨¡å‹è¿›è¡Œè¯„ä¼°
            response = self.llm.invoke([HumanMessage(content=evaluation_prompt)])
            response_text = response.content
            
            # è§£æè¯„ä¼°ç»“æœ
            quality_score, suggestions = self._parse_evaluation_response(response_text)
            
            logger.info(f"è¯„ä¼°å®Œæˆï¼Œæ€»åˆ†: {quality_score.total_score:.1f}")
            return quality_score, suggestions
            
        except Exception as e:
            logger.error(f"å¹»ç¯ç‰‡è´¨é‡è¯„ä¼°å¤±è´¥: {e}")
            # è¿”å›é»˜è®¤çš„ä½åˆ†è¯„ä¼°
            return self._create_fallback_evaluation(slide)

    def _build_evaluation_prompt(
        self, 
        slide: SlideContent, 
        outline: PresentationOutline,
        context_slides: Optional[List[SlideContent]] = None
    ) -> str:
        """æ„å»ºè´¨é‡è¯„ä¼°æç¤ºè¯"""
        
        context_info = ""
        if context_slides:
            context_info = "\n\n**ä¸Šä¸‹æ–‡å¹»ç¯ç‰‡ä¿¡æ¯:**\n"
            for i, ctx_slide in enumerate(context_slides[-3:]):  # æœ€å¤š3å¼ ä¸Šä¸‹æ–‡å¹»ç¯ç‰‡
                context_info += f"ç¬¬{i+1}å¼ : {ctx_slide.title} - {ctx_slide.main_content[:100]}...\n"

        prompt = f"""
ä½œä¸ºPPTè´¨é‡è¯„ä¼°ä¸“å®¶ï¼Œè¯·å¯¹ä»¥ä¸‹å¹»ç¯ç‰‡è¿›è¡Œå¤šç»´åº¦è´¨é‡è¯„åˆ†ï¼š

**æ¼”ç¤ºä¸»é¢˜:** {outline.title}
**ç›®æ ‡å—ä¼—:** {outline.target_audience if hasattr(outline, 'target_audience') else 'é€šç”¨å—ä¼—'}

**å¾…è¯„ä¼°å¹»ç¯ç‰‡:**
- æ ‡é¢˜: {slide.title}
- ç±»å‹: {slide.slide_type.value}
- ä¸»è¦å†…å®¹: {slide.main_content}
- è¦ç‚¹: {', '.join(slide.bullet_points) if slide.bullet_points else 'æ— '}
- æ³¨é‡Š: {slide.speaker_notes if slide.speaker_notes else 'æ— '}
{context_info}

**è¯„ä¼°ç»´åº¦å’Œæ ‡å‡†:**

1. **å†…å®¹é€»è¾‘æ€§ (30%)**
   - å†…å®¹ç»“æ„æ˜¯å¦æ¸…æ™°åˆç†
   - è®ºç‚¹è®ºæ®æ˜¯å¦å……åˆ†
   - é€»è¾‘æ¨ç†æ˜¯å¦ä¸¥å¯†

2. **ä¸»é¢˜ç›¸å…³æ€§ (25%)**
   - å†…å®¹æ˜¯å¦ä¸æ¼”ç¤ºä¸»é¢˜ç´§å¯†ç›¸å…³
   - æ˜¯å¦åç¦»ä¸»çº¿æˆ–è¿‡äºå‘æ•£
   - ä¸ä¸Šä¸‹æ–‡çš„è¿è´¯æ€§

3. **è¯­è¨€è¡¨è¾¾è´¨é‡ (25%)**
   - æ–‡å­—è¡¨è¾¾æ˜¯å¦æ¸…æ™°å‡†ç¡®
   - æ˜¯å¦ç®€æ´æœ‰åŠ›ï¼Œé¿å…å†—ä½™
   - ä¸“ä¸šæœ¯è¯­ä½¿ç”¨æ˜¯å¦æ°å½“

4. **è§†è§‰å¸ƒå±€åˆç†æ€§ (20%)**
   - ä¿¡æ¯å±‚æ¬¡æ˜¯å¦åˆ†æ˜
   - è¦ç‚¹æ•°é‡æ˜¯å¦é€‚ä¸­ï¼ˆå»ºè®®3-7ä¸ªï¼‰
   - å†…å®¹å¯†åº¦æ˜¯å¦åˆé€‚

**è¾“å‡ºæ ¼å¼:**
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºè¯„ä¼°ç»“æœï¼š

```json
{{
    "dimension_scores": {{
        "logic": åˆ†æ•°(0-100),
        "relevance": åˆ†æ•°(0-100), 
        "language": åˆ†æ•°(0-100),
        "layout": åˆ†æ•°(0-100)
    }},
    "confidence": ç½®ä¿¡åº¦(0-1),
    "optimization_suggestions": [
        {{
            "dimension": "ç»´åº¦åç§°",
            "issue_description": "é—®é¢˜æè¿°",
            "suggestion": "å…·ä½“æ”¹è¿›å»ºè®®",
            "priority": "ä¼˜å…ˆçº§(high/medium/low)"
        }}
    ]
}}
```

è¯·ç»™å‡ºå®¢è§‚ã€å‡†ç¡®çš„è¯„åˆ†å’Œå»ºè®®ã€‚
"""
        return prompt

    def _parse_evaluation_response(self, response_text: str) -> Tuple[QualityScore, List[OptimizationSuggestion]]:
        """è§£æAIè¯„ä¼°å“åº”"""
        try:
            import json
            import re
            
            # æå–JSONéƒ¨åˆ†
            json_match = re.search(r'```json\s*(\{.*?\})\s*```', response_text, re.DOTALL)
            if not json_match:
                # å°è¯•ç›´æ¥è§£ææ•´ä¸ªå“åº”
                json_str = response_text.strip()
            else:
                json_str = json_match.group(1)
            
            eval_data = json.loads(json_str)
            
            # è®¡ç®—æ€»åˆ†
            dimension_scores = eval_data["dimension_scores"]
            total_score = sum(
                score * self.dimension_weights.get(dim, 0.25) 
                for dim, score in dimension_scores.items()
            )
            
            # åˆ›å»ºè´¨é‡è¯„åˆ†å¯¹è±¡
            quality_score = QualityScore(
                total_score=round(total_score, 1),
                dimension_scores=dimension_scores,
                pass_threshold=total_score >= self.quality_threshold,
                confidence=eval_data.get("confidence", 0.8)
            )
            
            # åˆ›å»ºä¼˜åŒ–å»ºè®®åˆ—è¡¨
            suggestions = [
                OptimizationSuggestion(
                    dimension=sugg["dimension"],
                    issue_description=sugg["issue_description"],
                    suggestion=sugg["suggestion"],
                    priority=sugg["priority"]
                )
                for sugg in eval_data.get("optimization_suggestions", [])
            ]
            
            return quality_score, suggestions
            
        except Exception as e:
            logger.error(f"è§£æè¯„ä¼°å“åº”å¤±è´¥: {e}")
            logger.debug(f"åŸå§‹å“åº”: {response_text}")
            raise

    def _create_fallback_evaluation(self, slide: SlideContent) -> Tuple[QualityScore, List[OptimizationSuggestion]]:
        """åˆ›å»ºé™çº§è¯„ä¼°ç»“æœ"""
        logger.warning("ä½¿ç”¨é™çº§è¯„ä¼°ç»“æœ")
        
        quality_score = QualityScore(
            total_score=60.0,  # ä½äºé˜ˆå€¼çš„åˆ†æ•°
            dimension_scores={
                "logic": 60.0,
                "relevance": 60.0,
                "language": 60.0,
                "layout": 60.0
            },
            pass_threshold=False,
            confidence=0.3
        )
        
        suggestions = [
            OptimizationSuggestion(
                dimension="system",
                issue_description="è´¨é‡è¯„ä¼°ç³»ç»Ÿæš‚æ—¶ä¸å¯ç”¨",
                suggestion="è¯·æ‰‹åŠ¨æ£€æŸ¥å¹»ç¯ç‰‡å†…å®¹çš„é€»è¾‘æ€§ã€ç›¸å…³æ€§ã€è¯­è¨€è´¨é‡å’Œå¸ƒå±€åˆç†æ€§",
                priority="medium"
            )
        ]
        
        return quality_score, suggestions

    def should_regenerate(self, quality_score: QualityScore, retry_count: int) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦éœ€è¦é‡æ–°ç”Ÿæˆå¹»ç¯ç‰‡
        
        Args:
            quality_score: è´¨é‡è¯„åˆ†
            retry_count: å½“å‰é‡è¯•æ¬¡æ•°
            
        Returns:
            bool: æ˜¯å¦éœ€è¦é‡æ–°ç”Ÿæˆ
        """
        # å¦‚æœå·²è¾¾åˆ°åŠæ ¼çº¿ï¼Œä¸éœ€è¦é‡æ–°ç”Ÿæˆ
        if quality_score.pass_threshold:
            return False
            
        # å¦‚æœè¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œä¸å†é‡æ–°ç”Ÿæˆ
        if retry_count >= self.max_retry_count:
            logger.warning(f"å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° {self.max_retry_count}ï¼Œæ¥å—å½“å‰ç»“æœ")
            return False
            
        # è¯„åˆ†è¿‡ä½ä¸”è¿˜æœ‰é‡è¯•æœºä¼šï¼Œéœ€è¦é‡æ–°ç”Ÿæˆ
        logger.info(f"è´¨é‡è¯„åˆ† {quality_score.total_score:.1f} ä½äºé˜ˆå€¼ {self.quality_threshold}ï¼Œå°†è¿›è¡Œç¬¬ {retry_count + 1} æ¬¡é‡è¯•")
        return True

    def format_feedback_for_regeneration(
        self, 
        quality_score: QualityScore, 
        suggestions: List[OptimizationSuggestion]
    ) -> str:
        """
        æ ¼å¼åŒ–åé¦ˆä¿¡æ¯ç”¨äºé‡æ–°ç”Ÿæˆ
        
        Args:
            quality_score: è´¨é‡è¯„åˆ†
            suggestions: ä¼˜åŒ–å»ºè®®
            
        Returns:
            str: æ ¼å¼åŒ–çš„åé¦ˆä¿¡æ¯
        """
        feedback = f"**è´¨é‡è¯„ä¼°åé¦ˆ (å½“å‰å¾—åˆ†: {quality_score.total_score:.1f}/{self.quality_threshold})**\n\n"
        
        # æ·»åŠ å„ç»´åº¦å¾—åˆ†
        feedback += "**å„ç»´åº¦å¾—åˆ†:**\n"
        for dim, score in quality_score.dimension_scores.items():
            status = "âœ…" if score >= self.quality_threshold else "âŒ"
            feedback += f"- {dim}: {score:.1f}åˆ† {status}\n"
        
        # æ·»åŠ ä¼˜åŒ–å»ºè®®
        if suggestions:
            feedback += "\n**ä¼˜åŒ–å»ºè®®:**\n"
            high_priority = [s for s in suggestions if s.priority == "high"]
            medium_priority = [s for s in suggestions if s.priority == "medium"]
            
            if high_priority:
                feedback += "\nğŸ”´ **é«˜ä¼˜å…ˆçº§é—®é¢˜:**\n"
                for sugg in high_priority:
                    feedback += f"- {sugg.issue_description}\n  å»ºè®®: {sugg.suggestion}\n\n"
            
            if medium_priority:
                feedback += "ğŸŸ¡ **ä¸­ç­‰ä¼˜å…ˆçº§é—®é¢˜:**\n"
                for sugg in medium_priority:
                    feedback += f"- {sugg.issue_description}\n  å»ºè®®: {sugg.suggestion}\n\n"
        
        feedback += "\nè¯·æ ¹æ®ä»¥ä¸Šåé¦ˆä¼˜åŒ–å¹»ç¯ç‰‡å†…å®¹ï¼Œé‡ç‚¹å…³æ³¨å¾—åˆ†è¾ƒä½çš„ç»´åº¦ã€‚"
        
        return feedback